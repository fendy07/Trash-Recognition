{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557a9c60",
   "metadata": {},
   "source": [
    "# <b>Trash Recognition Using Deep Learning Model</b>\n",
    "\n",
    "## <b>Problem Statement</b>\n",
    "\n",
    "1. How can an algorithmic model identify waste objects based on their categories?\n",
    "2. Can models based on deep learning recognize trash objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0102906-070f-4a2e-adbb-ad33787eb017",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'trashnet\\dataset-resized'\n",
    "\n",
    "img_paths = []\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "            img_paths.append(os.path.join(root, file))\n",
    "\n",
    "labels = []\n",
    "for path in img_paths:\n",
    "    label = os.path.basename(os.path.dirname(path))\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038a39e",
   "metadata": {},
   "source": [
    "## <b>Exploratory Image Analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengonversi label menjadi bentuk one-hot encoding\n",
    "label_set = list(labels)\n",
    "label_dict = {label: i for i, label in enumerate(label_set)}\n",
    "labels_one_hot = [tf.one_hot(label_dict[label], len(label_set)) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc78b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_and_augmented_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels = 3)\n",
    "    image = tf.image.resize(image, [200, 200])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "    image = tf.image.random_contrast(image, lower = 0.9, upper = 1.1)\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "batch_size = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels_one_hot))\n",
    "dataset = dataset.map(load_and_augmented_image)\n",
    "dataset = dataset.shuffle(buffer_size = len(img_paths))\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Visualisasi beberapa gambar dari dataset\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in dataset.take(1):\n",
    "  for i in range(12):\n",
    "    ax = plt.subplot(5, 3, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    label = label_set[tf.argmax(labels[i])]\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan dataset menjadi dataset pelatihan dan validasi\n",
    "train_size = int(0.8 * len(img_paths))\n",
    "val_size = len(img_paths) - train_size\n",
    "\n",
    "train_dataset = dataset.take(train_size // batch_size)\n",
    "val_dataset = dataset.skip(train_size // batch_size)\n",
    "\n",
    "print(f\"Jumlah Data Pelatihan (Train): {train_size}\")\n",
    "print(f\"Jumlah data Validasi (Test): {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu', input_shape=(200, 200, 3)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(len(label_set), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EarlyStopping\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi gambaran model CNN\n",
    "import visualkeras\n",
    "\n",
    "visualkeras.layered_view(model,\n",
    "                         to_file='model_plot_cnn.png',\n",
    "                         show_dimension=True,\n",
    "                         legend=True) # write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompilasi model dan pelatihan model\n",
    "model.compile(Adam(learning_rate=0.01, weight_decay=0.04),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Histori model\n",
    "EPOCHS = 25\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch = train_size // batch_size,\n",
    "                    epochs = EPOCHS,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data = (val_dataset),\n",
    "                    verbose = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
